#!/usr/bin/env python3

import os
import sys
from os.path import isfile, join
from datetime import datetime

import toml
import requests
from scrapy.selector import Selector

from wiki_c.cli_message import success_message

URL_ROOT = 'https://doc.ubuntu-fr.org/'

def get_html_index():
    r = requests.get('%saccueil?do=index' % URL_ROOT)
    return r.text

def save_cache_index(index_raw):
    os.makedirs('cache', exist_ok=True)
    with open('cache/index.html', 'w') as f:
        f.write(index_raw)

def download_page(href):
    page = requests.get(URL_ROOT + href + '?do=edit')
    docuwiki = Selector(text=page.text).css('#wiki__text::text').get()
    with open('cache' + href + '.docuwiki', 'w') as f:
        f.write(docuwiki)

def recursive_link(a_dic):
    sub_sec = requests.post(
        'https://doc.ubuntu-fr.org/lib/exe/ajax.php',
        {
            'idx': a_dic['title'],
            'call': 'index'
        }
    )
    a_list = Selector(text=sub_sec.text).css('a').getall()
    for a in a_list:
        a_el = Selector(text=a).css('a')
        href = a_el.attrib['href']
        if '/' not in href:
            continue
        sub_a_dic = {
            'title': a_el.attrib['title'],
            'href': href,
            'is_dir': True if a_el.attrib['class'] == 'idx_dir' else False
        }
        if sub_a_dic['is_dir']:
            os.makedirs(
                'cache/' + sub_a_dic['title'].replace(':', '/'),
                exist_ok=True
            )
            recursive_link(sub_a_dic)
            continue
        download_page(sub_a_dic['href'])

def walk_links():
    with open('cache/index.html', 'r') as f:
        index_raw = f.read()
    a_list = Selector(text=index_raw).css('#index__tree a').getall()

    a_el_list = []

    for a in a_list:
        a_el = Selector(text=a).css('a')
        href = a_el.attrib['href']
        if '/' not in href:
            continue
        a_el_list.append({
            'title': a_el.attrib['title'],
            'href': href,
            'is_dir': True if a_el.attrib['class'] == 'idx_dir' else False
        })

    for a in a_el_list:
        if a['is_dir']:
            os.makedirs('cache/' + a['title'], exist_ok=True)
            recursive_link(a)
            continue
        download_page(a['href'])

def get_index():
    with open(join('cache', 'index.toml'), 'r') as f:
        index = toml.loads(f.read())
    return index

def update_index(index):
    index['last_date'] = datetime.now()
    with open(join('cache', 'index.toml'), 'w') as f:
        toml.dump(index, f)

def create_index():
    toml_str = """
    first_date = {0},
    last_date = {0}
    """.format(datetime.now())
    parsed_toml = toml.loads(toml_str)
    with open(join('cache', 'index.toml'), 'w') as f:
        toml.dump(parsed_toml, f)

def get_diff(last_date, index=0):
    nb_per_page = 40
    url = '{0}?do=recent&show_changes=pages&first%5B{1}%5D='.format(
        URL_ROOT, index * nb_per_page
    )
    page = requests.get(url)
    a_list = Selector(text=page.text).css('.fix-media-list-overlap .li').getall()

    for count, a in enumerate(a_list):
        txt_date = Selector(text=a).css('.date::text').get().strip()[3:]
        day = int(txt_date[0:2])
        month = int(txt_date[3:5])
        year = int(txt_date[6:10])
        hour = int(txt_date[12:14])
        minute = int(txt_date[15:17])
        page_date = datetime(year, month, day, hour, minute)

        if last_date > page_date:
            return

        href= None
        wikilink1 = Selector(text=a).css('.wikilink1')
        wikilink2 = Selector(text=a).css('.wikilink2')
        if 'href' in wikilink1.attrib:
            href = wikilink1.attrib['href']
        if 'href' in wikilink2.attrib:
            href = wikilink2.attrib['href']
        download_page(href)
    get_diff(last_date, index + 1)

if __name__ == '__main__':
    full = False
    if '-f' in sys.argv or '--full' in sys.argv:
        full = True
    if not full and isfile(join('cache', 'index.toml')):
        index = get_index()
        get_diff(index['last_date'])
        update_index(index)
        success_message('Recover up to date !')
        exit(0)
    index_raw = get_html_index()
    save_cache_index(index_raw)
    walk_links()
    create_index()
    success_message('Recover up to date !')
