#!/usr/bin/env python3

import os
import sys
from os.path import isfile, join
from datetime import datetime

import toml
import requests
from scrapy.selector import Selector

from wiki_c.cli_message import success_message

URL_ROOT = 'https://doc.ubuntu-fr.org/'

def get_html_index():
    r = requests.get('%saccueil?do=index' % URL_ROOT)
    return r.text

def save_cache_index(index_raw):
    with open('cache/index.html', 'w') as f:
        f.write(index_raw)

def download_page(href, force_update=False):
    path = 'cache' + href + '.dokuwiki'
    if not force_update and isfile(path):
        print(path, '[exist]')
        return
    print(path, '[created]')
    page = requests.get(URL_ROOT + href + '?do=edit')
    dokuwiki = Selector(text=page.text).css('#wiki__text::text').get()
    with open(path, 'w') as f:
        f.write(dokuwiki)

def recursive_link(a_dic, force_update=False):
    sub_sec = requests.post(
        'https://doc.ubuntu-fr.org/lib/exe/ajax.php',
        {
            'idx': a_dic['title'],
            'call': 'index'
        }
    )
    a_list = Selector(text=sub_sec.text).css('a').getall()
    for a in a_list:
        a_el = Selector(text=a).css('a')
        href = a_el.attrib['href']
        if '/' not in href:
            continue
        sub_a_dic = {
            'title': a_el.attrib['title'],
            'href': href,
            'is_dir': True if a_el.attrib['class'] == 'idx_dir' else False
        }
        if sub_a_dic['is_dir']:
            os.makedirs(
                'cache/' + sub_a_dic['title'].replace(':', '/'),
                exist_ok=True
            )
            recursive_link(sub_a_dic)
            continue
        download_page(sub_a_dic['href'])

def walk_links(force_update=False):
    with open('cache/index.html', 'r') as f:
        index_raw = f.read()
    a_list = Selector(text=index_raw).css('#index__tree a').getall()

    a_el_list = []

    for a in a_list:
        a_el = Selector(text=a).css('a')
        href = a_el.attrib['href']
        if '/' not in href:
            continue
        a_el_list.append({
            'title': a_el.attrib['title'],
            'href': href,
            'is_dir': True if a_el.attrib['class'] == 'idx_dir' else False
        })

    for a in a_el_list:
        if a['is_dir']:
            os.makedirs('cache/' + a['title'], exist_ok=True)
            recursive_link(a, force_update)
            continue
        download_page(a['href'], force_update)


class State:
    def __init__(self):
        self.path = join('cache', 'index.toml')
        os.makedirs('cache', exist_ok=True)

    def get_index(self):
        if not isfile(self.path):
            return None
        with open(self.path, 'r') as f:
            index = toml.loads(f.read())
        return index

    def update_index(self, index):
        index['last_date'] = datetime.now()
        index['finished'] = 'true'
        with open(self.path, 'w') as f:
            toml.dump(index, f)

    def create_index(self, force_update=False):
        index = self.get_index()
        if not force_update and index:
            return index

        toml_str = """
        first_date = {0},
        last_date = {0}
        finished = false
        """.format(datetime.now())
        index = toml.loads(toml_str)
        with open(self.path, 'w') as f:
            toml.dump(index, f)
        return index

    def is_finished(self):
        index = self.get_index()
        if not index:
            return False
        if index['finished'] == 'true':
            return True
        return False


def get_diff(last_date, index=0):
    nb_per_page = 40
    url = '{0}?do=recent&show_changes=pages&first%5B{1}%5D='.format(
        URL_ROOT, index * nb_per_page
    )
    page = requests.get(url)
    a_list = Selector(text=page.text).css('.fix-media-list-overlap .li').getall()

    for count, a in enumerate(a_list):
        txt_date = Selector(text=a).css('.date::text').get().strip()[3:]
        day = int(txt_date[0:2])
        month = int(txt_date[3:5])
        year = int(txt_date[6:10])
        hour = int(txt_date[12:14])
        minute = int(txt_date[15:17])
        page_date = datetime(year, month, day, hour, minute)

        if last_date > page_date:
            return

        href= None
        wikilink1 = Selector(text=a).css('.wikilink1')
        wikilink2 = Selector(text=a).css('.wikilink2')
        if 'href' in wikilink1.attrib:
            href = wikilink1.attrib['href']
        if 'href' in wikilink2.attrib:
            href = wikilink2.attrib['href']
        download_page(href, True)
    get_diff(last_date, index + 1)

if __name__ == '__main__':
    full = False
    state = State()

    if '-f' in sys.argv or '--full' in sys.argv:
        full = True

    if not full and state.is_finished():
        index = state.get_index()
        get_diff(index['last_date'])
        state.update_index(index)
        success_message('Recover up to date !')
        exit(0)

    index = state.create_index(full)
    index_raw = get_html_index()
    save_cache_index(index_raw)
    walk_links(full)
    get_diff(index['last_date'])
    state.update_index(index)
    success_message('Recover up to date !')
