#!/usr/bin/env python3

import os
import sys
from os.path import isfile, dirname
from datetime import datetime

import requests
from scrapy.selector import Selector

from wiki_c.cli_message import success_message, error_message
from wiki_c.state import State

URL_ROOT = 'https://doc.ubuntu-fr.org/'
DIR_DESTINATION = 'cache'


def get_html_index():
    r = requests.get('%saccueil?do=index' % URL_ROOT)
    return r.text


def save_cache_index(index_raw):
    if not index_raw:
        return
    with open('%s/index.html' % DIR_DESTINATION, 'w') as f:
        f.write(index_raw)


def download_page(href, force_update=False):
    path = DIR_DESTINATION + href + '.dokuwiki'
    if not force_update and isfile(path):
        print(path, '[exist]')
        return
    page = requests.get(URL_ROOT + href + '?do=edit')

    try:
        dokuwiki = Selector(text=page.text).css('#wiki__text::text').get()
        if dokuwiki:
            dokuwiki = dokuwiki.lstrip()
    except:
        error_message(href)

    if not dokuwiki:
        remove_page(href)
        return

    os.makedirs(dirname(path), exist_ok=True)
    print(path, '[created]')
    with open(path, 'w') as f:
        try:
            f.write(dokuwiki)
        except TypeError as e:
            print("TypeError: {0}".format(e))
            print("Url : {0}".format(page))
            print("content : {0}".format(page.text))
            exit(1)


def remove_page(href):
    path = DIR_DESTINATION + href + '.dokuwiki'
    if not isfile(path):
        return
    print(path, '[deleted]')
    os.remove(path)


def recursive_link(a_dic, force_update=False):
    sub_sec = requests.post(
        'https://doc.ubuntu-fr.org/lib/exe/ajax.php',
        {
            'idx': a_dic['title'],
            'call': 'index'
        }
    )
    a_list = Selector(text=sub_sec.text).css('a').getall()
    for a in a_list:
        a_el = Selector(text=a).css('a')
        href = a_el.attrib['href']
        if '/' not in href:
            continue
        sub_a_dic = {
            'title': a_el.attrib['title'],
            'href': href,
            'is_dir': True if a_el.attrib['class'] == 'idx_dir' else False
        }
        if sub_a_dic['is_dir']:
            os.makedirs(
                DIR_DESTINATION + '/' + sub_a_dic['title'].replace(':', '/'),
                exist_ok=True
            )
            recursive_link(sub_a_dic)
            continue
        download_page(sub_a_dic['href'])


def walk_links(force_update=False):
    with open('%s/index.html' % DIR_DESTINATION, 'r') as f:
        index_raw = f.read()
    a_list = Selector(text=index_raw).css('#index__tree a').getall()

    a_el_list = []

    deny_files = Path("deny_urls.txt").read_text(encoding="UTF-8")
    deny_list = deny_files.splitlines()

    for a in a_list:
        a_el = Selector(text=a).css('a')
        href = a_el.attrib['href']
        if '/' not in href:
            continue
        a_el_list.append({
            'title': a_el.attrib['title'],
            'href': href,
            'is_dir': True if a_el.attrib['class'] == 'idx_dir' else False
        })

    for a in a_el_list:
        if a['is_dir']:
            os.makedirs('%s/' % DIR_DESTINATION + a['title'], exist_ok=True)
            recursive_link(a, force_update)
            continue
        if a['href'] in deny_list:
            continue
        download_page(a['href'], force_update)


def get_hrefs(last_date, index=0, hrefs_dict = {}):
    nb_per_page = 40
    url = '{0}?do=recent&show_changes=pages&first%5B{1}%5D='.format(
        URL_ROOT, index * nb_per_page
    )
    page = requests.get(url)
    a_list = Selector(text=page.text).css('.fix-media-list-overlap .li').getall()

    for count, a in enumerate(a_list):
        txt_date = Selector(text=a).css('.date::text').get().strip()[3:]
        day = int(txt_date[0:2])
        month = int(txt_date[3:5])
        year = int(txt_date[6:10])
        hour = int(txt_date[12:14])
        minute = int(txt_date[15:17])
        page_date = datetime(year, month, day, hour, minute)

        if last_date > page_date:
            return

        href= None
        wikilink1 = Selector(text=a).css('.wikilink1')
        wikilink2 = Selector(text=a).css('.wikilink2')
        if 'href' in wikilink1.attrib:
            href = wikilink1.attrib['href']
        if 'href' in wikilink2.attrib:
            href = wikilink2.attrib['href']

        _sum = Selector(text=a).css('.sum::text').get()
        hrefs_dict[page_date] = {
            'href': href,
            'sum':  True if _sum == '\n – supprimée' else False
        }
    get_hrefs(last_date, index + 1, hrefs_dict)


def get_diff(last_date):
    hrefs_dict = {}
    get_hrefs(last_date, 0, hrefs_dict)

    for i in sorted(hrefs_dict.keys()):
        value = hrefs_dict[i]
        if value['sum']:
            remove_page(value['href'])
        else:
            download_page(value['href'], True)


if __name__ == '__main__':
    full = False
    state = State(DIR_DESTINATION)

    if '-f' in sys.argv or '--full' in sys.argv:
        full = True

    #download_page('/2048', True)
    #download_page('/0ad', True)
    #download_page('/farming_simulator_2015', True)
    #download_page('/conky_scripts_systeme', True)
    #download_page('/blender', True)
    #exit(0)

    if not full and state.is_finished():
        index = state.get_index()
        get_diff(index['last_date'])
        state.update_index(index)
        success_message('Recover up to date !')
        exit(0)

    index = state.create_index(full)
    index_raw = get_html_index()
    save_cache_index(index_raw)
    walk_links(full)
    get_diff(index['last_date'])
    state.update_index(index)
    success_message('Recover up to date !')
